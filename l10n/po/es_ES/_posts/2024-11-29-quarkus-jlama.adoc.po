msgid ""
msgstr ""
"Language: es_ES\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"X-Generator: jekyll-l10n\n"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "Creating pure Java LLM infused application with Quarkus, Langchain4j and Jlama"
msgstr "Creación de una aplicación Java LLM pura infundida con Quarkus, Langchain4j y Jlama"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "Creating pure Java LLM infused application with Quarkus, LangChain4j and Jlama"
msgstr "Creación de una aplicación Java pura con infusión LLM con Quarkus, LangChain4j y Jlama"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "Currently the vast majority of LLM-based applications rely on external services provided by specialized companies. These services typically offer the access to huge, general purpose models, implying energy consumption and then costs that are proportional to the size of these models."
msgstr "Actualmente, la gran mayoría de las aplicaciones basadas en LLM dependen de servicios externos proporcionados por empresas especializadas. Estos servicios suelen ofrecer el acceso a modelos enormes de propósito general, lo que implica un consumo de energía y unos costes proporcionales al tamaño de estos modelos."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "Even worse, this usage pattern also comes with both privacy and security concerns, since it is virtually impossible to be sure how those service providers will eventually re-use the prompts of their customers, which in some cases could also contain sensitive information."
msgstr "Peor aún, este patrón de uso también conlleva problemas de privacidad y seguridad, ya que es prácticamente imposible estar seguro de cómo esos proveedores de servicios reutilizarán finalmente las indicaciones de sus clientes, que en algunos casos también podrían contener información sensible."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "For these reasons many companies are exploring the option of training or fine-tuning smaller models that do not claim to be usable in a general context, but that are tailored towards specific business needs and subsequently running (serving in LLM terms) these models on premise or on private clouds."
msgstr "Por estas razones, muchas empresas están explorando la opción de formar o poner a punto modelos más pequeños que no pretenden ser utilizables en un contexto general, sino que están adaptados a necesidades empresariales específicas y, posteriormente, ejecutar (servir en términos de LLM) estos modelos in situ o en nubes privadas."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "The features provided by these specialized models need to be integrated into the existing software infrastructure, that in the enterprise world are very often written in Java. This could be accomplished following a traditional client-server architecture, for instance serving the model through an external server like https://ollama.com/[Ollama] and querying it through REST calls. While this should not present any particular problem for Java developers, they could work more efficiently, if they could consume the model directly in Java and without any need to install additional tools. Finally the possibility of embedding the LLM interaction directly in the same Java process running the application will make it easier to move from local dev to deployment, relieving IT from the burden of managing an external server, thus bypassing the need for a more mature platform engineering strategy. This is where Jlama comes into play."
msgstr "Las funciones que ofrecen estos modelos especializados deben integrarse en la infraestructura de software existente, que en el mundo empresarial suele estar escrita en Java. Esto podría lograrse siguiendo una arquitectura cliente-servidor tradicional, por ejemplo sirviendo el modelo a través de un servidor externo como link:https://ollama.com/[Ollama] y consultándolo mediante llamadas REST. Aunque esto no debería suponer ningún problema especial para los desarrolladores Java, podrían trabajar de forma más eficiente si pudieran consumir el modelo directamente en Java y sin necesidad de instalar herramientas adicionales. Por último, la posibilidad de incrustar la interacción LLM directamente en el mismo proceso Java que ejecuta la aplicación facilitará el paso del desarrollo local al despliegue, liberando al departamento de TI de la carga de gestionar un servidor externo, evitando así la necesidad de una estrategia de ingeniería de plataformas más madura. Aquí es donde entra en juego Jlama."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "How and why executing LLM inference in pure Java with Jlama"
msgstr "Cómo y por qué ejecutar la inferencia LLM en Java puro con Jlama"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "https://github.com/tjake/Jlama[Jlama] is a library allowing to execute LLM inference in pure Java. It supports many LLM model families like Llama, Mistral, Qwen2 and Granite. It also implements out-of-the-box many useful LLM related features like functions calling, models quantization, mixture of experts and even distributed inference."
msgstr "link:https://github.com/tjake/Jlama[Jlama] es una biblioteca que permite ejecutar la inferencia LLM en Java puro. Soporta muchas familias de modelos LLM como Llama, Mistral, Qwen2 y Granite. También implementa out-of-the-box muchas características útiles relacionadas con LLM como la llamada a funciones, la cuantización de modelos, la mezcla de expertos e incluso la inferencia distribuida."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "Jlama is well integrated with Quarkus through the https://quarkus.io/extensions/io.quarkiverse.langchain4j/quarkus-langchain4j-jlama/[dedicated lanchain4j based extension]. Note that for performance reasons Jlama uses the https://openjdk.org/jeps/469[Vector API] which is still in preview in Java 23, and very likely will be released as a supported feature in Java 25."
msgstr "Jlama está bien integrado con Quarkus a través de la link:https://quarkus.io/extensions/io.quarkiverse.langchain4j/quarkus-langchain4j-jlama/[extensión dedicada basada en lanchain4j] . Tenga en cuenta que, por razones de rendimiento, Jlama utiliza la link:https://openjdk.org/jeps/469[API de vectores] , que aún está en fase de vista previa en Java 23, y que muy probablemente se lanzará como característica compatible en Java 25."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "In essence Jlama makes it possible to serve an LLM in Java, directly embedded in the same JVM running your Java application, but why could this be useful? Actually this is desirable in many use cases and presents a number of relevant advantages like the following:"
msgstr "En esencia, Jlama hace posible servir un LLM en Java, directamente incrustado en la misma JVM que ejecuta su aplicación Java, pero ¿por qué podría ser esto útil? En realidad esto es deseable en muchos casos de uso y presenta una serie de ventajas relevantes como las siguientes:"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Similar lifecycle between model and app*: There can be use cases where the model and the application using it have the same lifecycle, so that the development of a new feature in the application also requires a change in the model. Similarly, since prompts are very dependent on the model, when the model gets updated even through fine-tuning, your prompt may need to be replaced. In these situations having the model embedded in the application will contribute to simplify the versioning and traceability of the development cycle."
msgstr "*Ciclo de vida similar entre el modelo y la aplicación* Puede haber casos de uso en los que el modelo y la aplicación que lo utiliza tengan el mismo ciclo de vida, de modo que el desarrollo de una nueva característica en la aplicación requiera también un cambio en el modelo. Del mismo modo, dado que los avisos dependen en gran medida del modelo, cuando éste se actualiza, incluso mediante un ajuste fino, puede ser necesario sustituir su aviso. En estas situaciones, tener el modelo incrustado en la aplicación contribuirá a simplificar el versionado y la trazabilidad del ciclo de desarrollo."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Fast development/prototyping*: Not having to install, configure and interact with an external server can make the development of a LLM-based Java application much easier."
msgstr "*Rápido desarrollo/prototipado* : No tener que instalar, configurar e interactuar con un servidor externo puede facilitar enormemente el desarrollo de una aplicación Java basada en LLM."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Easy models testing*: Running the LLM inference embedded in the JVM also makes it easier to test different models and their integration during the development phase."
msgstr "*Facilidad para probar modelos* : La ejecución de la inferencia LLM embebida en la JVM también facilita la prueba de diferentes modelos y su integración durante la fase de desarrollo."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Security*: Performing the model inference in the same JVM instance that is run the application using it, eliminates the need of interacting with the LLM only through REST calls, thus preventing the leak of private data and allowing to enforce user authorization at a much finer grain."
msgstr "*Seguridad* : Al realizar la inferencia del modelo en la misma instancia JVM que ejecuta la aplicación que lo utiliza, se elimina la necesidad de interactuar con el LLM únicamente a través de llamadas REST, evitando así la fuga de datos privados y permitiendo hacer cumplir la autorización de los usuarios a un grano mucho más fino."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Monolithic applications support*: The former point will be also beneficial for users still running monolithic applications, who in this way will be also able to include LLM-based capabilities in those applications without changing their architecture or platform."
msgstr "*Soporte de aplicaciones monolíticas* : El punto anterior también será beneficioso para los usuarios que aún ejecuten aplicaciones monolíticas, que de este modo también podrán incluir capacidades basadas en LLM en dichas aplicaciones sin cambiar su arquitectura o plataforma."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Monitoring and Observability*: Running the LLM inference in pure Java will also allow simplify monitoring and observability, gathering statistics on the reliability and speed of the LLM response."
msgstr "*Monitorización y observabilidad* : Ejecutar la inferencia LLM en Java puro también permitirá simplificar la monitorización y la observabilidad, recopilando estadísticas sobre la fiabilidad y la velocidad de la respuesta LLM."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Developer Experience*: Debuggability will be simplified in the same way, allowing the Java developer to also navigate and debug the Jlama code if necessary."
msgstr "*Experiencia del desarrollador* : La depuración se simplificará del mismo modo, permitiendo al desarrollador Java navegar y depurar también el código Jlama si es necesario."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Distribution*: Having the possibility to run LLM inference embedded in the same Java process will also make it possible to include the model itself into the same application package of the application using it (even though this could probably be advisable only in very specific circumstances)."
msgstr "*Distribución* : Tener la posibilidad de ejecutar la inferencia LLM incrustada en el mismo proceso Java también hará posible incluir el propio modelo en el mismo paquete de la aplicación que lo utilice (aunque probablemente esto sólo sea aconsejable en circunstancias muy específicas)."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Edge friendliness*: The possibility of implementing and deploying a self-contained LLM-capable Java application will also make it a better fit than a client/server architecture for edge environments."
msgstr "*Facilidad para el borde* : La posibilidad de implementar y desplegar una aplicación Java autónoma compatible con LLM también hará que se adapte mejor que una arquitectura cliente/servidor a los entornos de borde."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Embedding of auxiliary LLMs*: Many applications, especially the ones relying on agentic AI patterns, uses many different LLMs at once. For instance a smaller LLM could be used to validate and approve the responses of the main bigger one. In this case an hybrid approach could be convenient, embedding the smaller auxiliary LLMs while keeping serving the main one through a dedicated server."
msgstr "*Incrustación de LLM auxiliares* : Muchas aplicaciones, especialmente las que se basan en patrones de IA agéntica, utilizan muchos LLM diferentes a la vez. Por ejemplo, un LLM más pequeño podría utilizarse para validar y aprobar las respuestas del principal más grande. En este caso podría ser conveniente un enfoque híbrido, incrustando los LLM auxiliares más pequeños mientras se sigue sirviendo al principal a través de un servidor dedicado."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "The site summarizer: a pure Java LLM-based application"
msgstr "El resumidor de sitios: una aplicación basada en Java LLM puro"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "To demonstrate how Quarkus, Langchain4j and Jlama make straightforward to create a pure Java LLM infused application, where the LLM inference is directly embedded in the same JVM running the application I created a https://github.com/mariofusco/site-summarizer[simple project] that uses a LLM to automatically generate the summarization of a Wikipedia page or more in general of a blog post taken from any website."
msgstr "Para demostrar cómo Quarkus, Langchain4j y Jlama facilitan la creación de una aplicación Java pura infundida con LLM, en la que la inferencia LLM está directamente incrustada en la misma JVM que ejecuta la aplicación, he creado un link:https://github.com/mariofusco/site-summarizer[proyecto sencillo] que utiliza un LLM para generar automáticamente el resumen de una página de Wikipedia o, más en general, de una entrada de blog tomada de cualquier sitio web."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "This video demonstrates how the application works."
msgstr "Este vídeo muestra cómo funciona la aplicación."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "Out-of-the-box this project uses a https://huggingface.co/tjake/Llama-3.2-1B-Instruct-JQ4[small Llama-3.2 model with 4-bit quantization]. When the application is compiled for the first time the model is automatically downloaded locally by Jlama from the Huggingface repository. However it is possible to replace this model and experiment with any other one by simply editing the https://github.com/mariofusco/site-summarizer/blob/main/src/main/resources/application.properties#L4[quarkus.langchain4j.jlama.chat-model.model-name property] in the application.properties file."
msgstr "Out-of-the-box este proyecto utiliza un link:https://huggingface.co/tjake/Llama-3.2-1B-Instruct-JQ4[pequeño modelo Llama-3.2 con cuantificación de 4 bits] . Cuando se compila la aplicación por primera vez, Jlama descarga automáticamente el modelo de forma local desde el repositorio Huggingface. Sin embargo, es posible sustituir este modelo y experimentar con cualquier otro simplemente editando la link:https://github.com/mariofusco/site-summarizer/blob/main/src/main/resources/application.properties#L4[propiedad quarkus.langchain4j.jlama.chat-model.model-name] en el archivo application.properties."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "After having compiled and packaged the project with `mvn clean package`, the simplest way to use it is launching the jar passing as argument the URL of the web page containing the article that you want to summarize, something like:"
msgstr "Después de haber compilado y empaquetado el proyecto con `mvn clean package` , la forma más sencilla de utilizarlo es lanzando el jar pasando como argumento la URL de la página web que contiene el artículo que se quiere resumir, algo así como"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "that will generate an output like the following:"
msgstr "que generará una salida como la siguiente:"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "Note that it is necessary to launch the JVM with a few additional arguments that enable the access to the Vector API which is still a Java preview feature, but it is internally used by Jlama to speed up the computation."
msgstr "Tenga en cuenta que es necesario lanzar la JVM con algunos argumentos adicionales que habilitan el acceso a la API de vectores, que aún es una característica previa de Java, pero que Jlama utiliza internamente para acelerar el cálculo."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "As better clarified by the readme of the project, there's a dedicated operating mode to process Wikipedia pages and also the possibility to expose this service through a REST endpoint."
msgstr "Como se aclara mejor en el readme del proyecto, existe un modo de funcionamiento dedicado para procesar las páginas de Wikipedia y también la posibilidad de exponer este servicio a través de un endpoint REST."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "The internal implementation of this project is relatively straightforward: after having programmatically extracted the text to be summarized from the HTML page containing it, that text is sent to Jlama to be processed via a usual Langchain4j AiService."
msgstr "La implementación interna de este proyecto es relativamente sencilla: tras haber extraído mediante programación el texto a resumir de la página HTML que lo contiene, dicho texto se envía a Jlama para que lo procese mediante un AiService Langchain4j habitual."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "As anticipated, despite this implementation looks identical to any other LLM inference engine integrations, in this case there isn't any remote call to an external service, but the LLM inference is performed directly inside the same JVM running the application."
msgstr "Como anticipamos, a pesar de que esta implementación parece idéntica a cualquier otra integración del motor de inferencia LLM, en este caso no hay ninguna llamada remota a un servicio externo, sino que la inferencia LLM se realiza directamente dentro de la misma JVM que ejecuta la aplicación."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "The combination of the 2 trends of the increasing spread of small and tailored models and the adoption of these models in the enterprise software development world will very likely promote the use of similar solutions in the near future."
msgstr "La combinación de las 2 tendencias de la creciente difusión de modelos pequeños y a medida y la adopción de estos modelos en el mundo del desarrollo de software empresarial promoverá muy probablemente el uso de soluciones similares en un futuro próximo."
